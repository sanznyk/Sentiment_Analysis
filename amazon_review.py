# -*- coding: utf-8 -*-
"""Amazon_Review.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XyAiwLj4mXiVbKS9UdH7S3yTMxDMm9hi
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("/content/amazon_reviews.csv")

df

print(df.columns)

# 1. Review Count
total_reviews = df['reviews.text'].notnull().sum()
print(f"Total reviews: {total_reviews}")

# 2. Review Lengths (in characters & words)
df['review_length_chars'] = df['reviews.text'].fillna("").apply(len)
df['review_length_words'] = df['reviews.text'].fillna("").apply(lambda x: len(x.split()))

avg_len_chars = df['review_length_chars'].mean()
avg_len_words = df['review_length_words'].mean()

print(f"Average review length (characters): {avg_len_chars:.2f}")
print(f"Average review length (words): {avg_len_words:.2f}")

# 3. Rating Distribution ---
print("\nRating Distribution:")
print(df['reviews.rating'].value_counts().sort_index())

# Optional: visualize rating distribution with a bar chart
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
df['reviews.rating'].value_counts().sort_index().plot(kind='bar', color='skyblue')
plt.title('Review Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# 4. Optional: Missing Data ---
print("\nMissing values per column:")
print(df[['reviews.text', 'reviews.rating']].isnull().sum())

import re
import nltk
from nltk.corpus import stopwords
import nltk
nltk.download('punkt_tab')

# Download stopwords if not already done
nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))

def clean_text_basic(text):
    if pd.isnull(text):
        return ""

    # Lowercase
    text = text.lower()

    # Remove punctuation, special characters, and numbers
    text = re.sub(r'[^a-z\s]', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenize
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    return ' '.join(tokens)

df['cleaned_text'] = df['reviews.text'].apply(clean_text_basic)

df[['reviews.text', 'cleaned_text']].head()

from nltk.stem import PorterStemmer
import nltk

# Initialize the stemmer
stemmer = PorterStemmer()

# Define the stemming function
def stem_text(text):
    words = text.split()
    stemmed_words = [stemmer.stem(word) for word in words]
    return " ".join(stemmed_words)

# Apply stemming to the cleaned reviews text
df['stemmed_text'] = df['cleaned_text'].apply(stem_text)

print(df[['cleaned_text', 'stemmed_text']].head())

import nltk
nltk.download('punkt')       # For word_tokenize
nltk.download('wordnet')     # For lemmatizer
nltk.download('omw-1.4')     # Optional, but improves lemmatization

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Define the lemmatization function
def lemmatize_text(text):
    words = word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return " ".join(lemmatized_words)

# Apply lemmatization to the cleaned reviews text
df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)

print(df[['cleaned_text', 'lemmatized_text']].head())

df.to_csv("cleaned_reviews.csv", index=False)

def get_sentiment(rating):
    if pd.isnull(rating):
        return "unknown"  # optional: handle missing ratings
    elif rating >= 4:
        return "positive"
    elif rating == 3:
        return "neutral"
    else:
        return "negative"

df['sentiment'] = df['reviews.rating'].apply(get_sentiment)

print(df[['reviews.rating', 'sentiment']].tail())

import matplotlib.pyplot as plt

df['sentiment'].value_counts().plot(kind='bar', color='lightgreen', title='Sentiment Distribution')
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer(max_features=5000)
X_count = count_vec.fit_transform(df['cleaned_text'])

print("CountVectorizer shape:", X_count.shape)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vec = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_vec.fit_transform(df['cleaned_text'])

print("TF-IDF shape:", X_tfidf.shape)

from sklearn.model_selection import train_test_split

# Split the data into train and test sets (80% train, 20% test)
X = X_count  # Features (text data converted to count vector)
y = df['sentiment']  # Target variable (sentiment labels)

# Train-test split (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Initialize Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, random_state=42)

# Train the model
log_reg.fit(X_train, y_train)

# Predict on test set
y_pred_log_reg = log_reg.predict(X_test)

# Evaluate model
print("Logistic Regression Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_log_reg):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_log_reg, average='weighted'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_log_reg, average='weighted'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_log_reg, average='weighted'):.4f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_log_reg)}\n")

from sklearn.naive_bayes import MultinomialNB

# Initialize Naive Bayes model
nb = MultinomialNB()

# Train the model
nb.fit(X_train, y_train)

# Predict on test set
y_pred_nb = nb.predict(X_test)

# Evaluate model
print("Naive Bayes Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_nb, average='weighted'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_nb, average='weighted'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_nb, average='weighted'):.4f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_nb)}\n")

from sklearn.svm import SVC

# Initialize SVM model
svm = SVC(random_state=42)

# Train the model
svm.fit(X_train, y_train)

# Predict on test set
y_pred_svm = svm.predict(X_test)

# Evaluate model
print("Support Vector Machine Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_svm, average='weighted'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_svm, average='weighted'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_svm, average='weighted'):.4f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_svm)}\n")

from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest model
rf = RandomForestClassifier(random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Predict on test set
y_pred_rf = rf.predict(X_test)

# Evaluate model
print("Random Forest Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_rf, average='weighted'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_rf, average='weighted'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_rf, average='weighted'):.4f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_rf)}\n")

import pickle

# Save the model to a file
with open('model.pkl', 'wb') as file:
    pickle.dump(log_reg, file)

with open('vectorizer.pkl', 'wb') as f:
    pickle.dump(count_vec, f)

